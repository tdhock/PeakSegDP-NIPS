\documentclass[legalpaper]{article}
\usepackage{tikz}
%\usepackage{fullpage}
\usepackage{amsmath,amssymb}
\newcommand{\ZZ}{\mathbb Z}
\usepackage{color}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Segments}{Segments}

\begin{document}

\section*{The Supervised Peak Detection Problem}

\begin{itemize}
\item A ChIP-seq profile on a single
chromosome with $d$ base pairs\\ is a vector $\mathbf y=
\left[
  \begin{array}{ccc}
    y_1 & \cdots & y_d
  \end{array}
\right]\in\ZZ_+^d$ of counts of aligned sequence reads. 
\item A peak caller is a function $c:\ZZ_+^d
  \rightarrow \{0, 1\}^d$\\
  which returns 0 for background noise and 1 for a peak.
\item We are given $i\in\{1,\dots, n\}$ profiles 
  $\mathbf y_i$ and annotated region labels $L_i$.
\item The goal is to find a peak caller with minimal error on some
test profiles:
\begin{equation*}
  \label{eq:min_error}
  \minimize_c \sum_{i\in\text{test}} E[c(\mathbf y_i),  L_i],
\end{equation*}
where $E$ is the number of false positive and false negative labels $L_i$.
\end{itemize}

\newpage 

\section*{PeakSeg: constrained Poisson segmentation}

\begin{equation*}
  \label{argmin:constrained}
  \begin{aligned}
    \mathbf{\tilde m}^s(\mathbf y)  =\ 
    &\argmin_{\mathbf m\in\RR^{d}} && 
    %\sum_{j=1}^d \log\Lik
    \text{PoissonLoss}
    (\mathbf m, \mathbf y) \\
    \\
    &\text{such that} && \Segments(\mathbf m)=s,\\
    \textbf{Peaks constraint:}
    & && P_j(\mathbf m) \in\{0, 1\} \text{ for all } j\in\{2, \dots, d\}.
  \end{aligned}
\end{equation*}
\begin{itemize}
\item$\text{PoissonLoss}(\mathbf m, \mathbf y)=
  \sum_{j=1}^d m_j - y_j \log m_j$.
\item$\Segments(\mathbf m)=\sum_{j=2}^d I(m_j \neq m_{j-1})\in\{1, \dots, d\}$ is the number of
  piecewise constant segments of the mean vector $\mathbf m$.
\item The indicator for a peak at base $j$ is 
$
  P_j(\mathbf m) = \sum_{k=1}^j \sign( m_k - m_{k-1} ).
$
\item Geometric interpretation of \textbf{Peaks constraint}:\\
  segment mean must change up, down, up, down, ...
\item For example for $s=5$ segments, mean values should satisfy\\
  $\mu_1 < \mu_2 > \mu_3 < \mu_4 > \mu_5$.
\end{itemize}

\newpage

\section*{Supervised penalty learning}
Reference: Hocking, Rigaill, et al. Learning Sparse Penalties for
Change-point Detection using Max Margin Interval Regression. ICML
2013.

\begin{itemize}
\item Train on count data $\mathbf y_i\in\ZZ_+^{d_i}$ and labels $L_i$
  for $i$ profiles.
\item For a positive penalty $\lambda\in\RR_+$, the optimal number
  of segments is
\begin{equation*} s^*(\lambda, \mathbf y) =
    \argmin_{s\in\{1,3,\dots, s_{\text{max}}\}} \text{PoissonLoss}\left[
      \mathbf{\tilde m}^s(\mathbf y), \mathbf y \right] + \lambda h(s, d).
\end{equation*}
\item Model complexity $h(s, d_i)$ is either AIC/BIC or oracle (see
  table below).
\item Profile-specific penalty values $\log \lambda_i = 
  f(\mathbf x_i)
  = \beta + \mathbf w^\intercal \mathbf x_i
$.
\item Learn the penalty $f$ with minimal incorrect regions $L_i$ on a
  train data set.
\item At test time compute the predicted penalty $\hat \lambda = \exp
  \hat f(\mathbf x)$ and the predicted number of segments $\hat s =
  s^*(\hat \lambda, \mathbf y)$.
\end{itemize}

\newpage


  \begin{minipage}{0.5\textwidth}
\hskip -5cm
  \begin{tabular}{ll}
    \textbf{name} & \textbf{model complexity} $h(s, d_i)$ \\
    \hline
    AIC/BIC.* & $s$\\
    oracle.* & $s\left(1 + 4\sqrt{1.1 + \log(d_i/s)}\right)^2$
  \end{tabular}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
\hskip -4cm
  \begin{tabular}{lllll}
    \textbf{name} & \textbf{learned} $\lambda_i$ & 
    \textbf{parameters} & \textbf{learning algorithm} \\
    \hline
    *.0 & AIC=2, BIC=$\log d_i$ & none & unsupervised \\
    *.1 & 
    $\beta$ & 
    $\beta\in\RR_+$ & grid search \\
    *.3 & 
    $e^\beta d_i^{w_1} (\max \mathbf y_i)^{w_{2}}$ & 
    $\beta, w_1, w_{2}\in\RR$ & interval regression \\
    *.41 & 
    $\exp(\beta + \mathbf w^\intercal \mathbf x_i)$ & 
    $\beta\in\RR, \mathbf w\in\RR^{40}$ & 
    regularized int. reg. \\
  \end{tabular}
  \end{minipage}



\end{document}
